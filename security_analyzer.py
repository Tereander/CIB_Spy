"""
–ú–æ–¥—É–ª—å –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü.

–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–º–µ–Ω–æ–≤, SSL, –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ —Å–∞–π—Ç–æ–≤.
"""

import re
import socket
import ssl
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import urlparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError

import requests
from bs4 import BeautifulSoup
from OpenSSL import crypto
import whois
from whois.parser import PywhoisError

from config import Config
from logs import logger


def check_domain_and_ssl(url: str) -> Dict[str, Any]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ–º–µ–Ω –∏ SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç.

    Args:
        url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

    Returns:
        Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–æ–º–µ–Ω–µ –∏ SSL
    """
    result = {
        'domain_age_days': None,
        'ssl_valid_days': None,
        'is_suspicious_domain': False
    }

    try:
        domain = urlparse(url).netloc
        if not domain:
            return result

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ WHOIS —Å —Ç–∞–π–º–∞—É—Ç–æ–º
        try:
            with ThreadPoolExecutor() as executor:
                future = executor.submit(whois.whois, domain)
                domain_info = future.result(timeout=Config.WHOIS_TIMEOUT)

                if domain_info and domain_info.creation_date:
                    creation_date = min(domain_info.creation_date) if isinstance(
                        domain_info.creation_date, list) else domain_info.creation_date
                    if creation_date:
                        age_days = (datetime.now() - creation_date).days
                        result['domain_age_days'] = age_days
                        if age_days < 30:
                            result['is_suspicious_domain'] = True
        except (FutureTimeoutError, PywhoisError, AttributeError) as e:
            logger.warning(f"WHOIS timeout/error for {domain}: {str(e)}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ SSL
        try:
            hostname = domain.split(':')[0]
            context = ssl.create_default_context()

            with socket.create_connection((hostname, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert(binary_form=True)
                    x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, cert)
                    expire_bytes = x509.get_notAfter()

                    if expire_bytes:
                        expire_str = expire_bytes.decode('ascii')
                        expire_date = datetime.strptime(expire_str, '%Y%m%d%H%M%SZ')
                        result['ssl_valid_days'] = (expire_date - datetime.now()).days
        except Exception as e:
            logger.warning(f"SSL check error for {hostname}: {str(e)}")

    except Exception as e:
        logger.error(f"Domain/SSL check critical error: {str(e)}")

    return result


def analyze_content(soup: BeautifulSoup) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã.

    Args:
        soup: BeautifulSoup –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """
    result = {
        'fake_login_forms': 0,
        'external_scripts': [],
        'iframe_count': 0,
        'popup_elements': 0
    }

    # –ü–æ–∏—Å–∫ –ø–æ–¥–¥–µ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º –≤—Ö–æ–¥–∞
    for form in soup.find_all('form'):
        if (form.find('input', {'type': 'password'}) and
            not form.find('input', {'type': 'submit'})):
            result['fake_login_forms'] += 1

    # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç—ã
    for script in soup.find_all('script', src=True):
        if ('jquery' not in script['src'].lower() and
            'google' not in script['src'].lower()):
            result['external_scripts'].append(script['src'][:100])

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ iframe
    result['iframe_count'] = len(soup.find_all('iframe'))

    # –≠–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è popup
    result['popup_elements'] = len(
        soup.find_all(onclick=True) +
        soup.find_all(onmouseover=True)
    )

    return result


def check_reputation(url: str) -> Dict[str, Any]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ø—É—Ç–∞—Ü–∏—é —Å–∞–π—Ç–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–∞—Ö.

    Args:
        url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏
    """
    result = {
        'google_safe_browsing': None,
        'phishtank_status': None,
        'is_blacklisted': False
    }

    try:
        # Google Safe Browsing
        if (hasattr(Config, 'GOOGLE_SAFE_BROWSING_KEY') and
            Config.GOOGLE_SAFE_BROWSING_KEY):
            try:
                payload = {
                    "client": {"clientId": "security-bot", "clientVersion": "1.0"},
                    "threatInfo": {
                        "threatTypes": ["MALWARE", "SOCIAL_ENGINEERING"],
                        "platformTypes": ["ANY_PLATFORM"],
                        "threatEntryTypes": ["URL"],
                        "threatEntries": [{"url": url}]
                    }
                }
                response = requests.post(
                    f"https://safebrowsing.googleapis.com/v4/threatMatches:find"
                    f"?key={Config.GOOGLE_SAFE_BROWSING_KEY}",
                    json=payload,
                    timeout=10
                )
                result['google_safe_browsing'] = bool(response.json().get('matches'))
            except Exception as e:
                logger.warning(f"Google Safe Browsing error: {str(e)}")

        # PhishTank –ø—Ä–æ–≤–µ—Ä–∫–∞
        try:
            phishtank_response = requests.get(
                f"https://checkurl.phishtank.com/checkurl/?url={url}",
                headers={'User-Agent': 'SecurityBot'},
                timeout=10
            )
            if phishtank_response.ok:
                result['phishtank_status'] = 'phish' in phishtank_response.text.lower()
        except Exception as e:
            logger.warning(f"PhishTank error: {str(e)}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º blacklist —Å—Ç–∞—Ç—É—Å
        safe_browsing = result['google_safe_browsing']
        phishtank = result['phishtank_status']

        if safe_browsing is not None or phishtank is not None:
            result['is_blacklisted'] = (
                (safe_browsing is True) or
                (phishtank is True)
            )

    except Exception as e:
        logger.error(f"Reputation check critical error: {str(e)}")

    return result


def heuristic_checks(url: str, page_source: str) -> Dict[str, Any]:
    """
    –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —Ñ–∏—à–∏–Ω–≥ –∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.

    Args:
        url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        page_source: –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
    """
    result = {
        'typosquatting': False,
        'brand_impersonation': False,
        'suspicious_keywords': 0
    }

    # –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    suspicious_keywords = ['login', 'password', 'verify', 'account', 'secure']
    for keyword in suspicious_keywords:
        if keyword in page_source.lower():
            result['suspicious_keywords'] += 1

    return result


class SecurityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü."""

    @staticmethod
    def analyze_page_security(driver: Any, url: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ñ–∏—à–∏–Ω–≥–∞.

        Args:
            driver: WebDriver —ç–∫–∑–µ–º–ø–ª—è—Ä
            url: URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        """
        analysis = {
            'basic_checks': {
                'has_ssl': url.startswith('https'),
                'domain': urlparse(url).netloc,
                'page_title': driver.title[:100]
            },
            'phishing_indicators': {
                'hidden_elements': 0,
                'password_fields': 0,
                'suspicious_scripts': 0,
                'external_resources': []
            },
            'warnings': [],
            'is_dangerous': False,
            'domain_checks': {},
            'content_analysis': {},
            'reputation': {},
            'heuristics': {}
        }

        try:
            soup = BeautifulSoup(driver.page_source, 'html.parser')

            # 1. –ü–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            hidden_elements = []
            for tag in soup.find_all(style=True):
                if re.search(
                    r'display:\s*none|visibility:\s*hidden',
                    tag['style'],
                    re.IGNORECASE
                ):
                    hidden_elements.append(tag)
            analysis['phishing_indicators']['hidden_elements'] = len(hidden_elements)

            # 2. –ü–æ–∏—Å–∫ –ø–æ–ª–µ–π –ø–∞—Ä–æ–ª—è
            analysis['phishing_indicators']['password_fields'] = len(
                soup.find_all('input', {'type': 'password'})
            )

            # 3. –ü–æ–∏—Å–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤
            dangerous_terms = [r'eval\(', r'fromCharCode', r'atob\(']
            suspicious_scripts = []
            for script in soup.find_all('script'):
                if script.string:
                    for term in dangerous_terms:
                        if re.search(term, script.string, re.IGNORECASE):
                            suspicious_scripts.append(script)
                            break
            analysis['phishing_indicators']['suspicious_scripts'] = len(suspicious_scripts)

            # 4. –ê–Ω–∞–ª–∏–∑ –≤–Ω–µ—à–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
            domain = urlparse(url).netloc
            for tag in soup.find_all(['img', 'script', 'link', 'iframe']):
                src = tag.get('src') or tag.get('href')
                if src and domain not in src:
                    analysis['phishing_indicators']['external_resources'].append(src[:200])

            # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ñ–∏—à–∏–Ω–≥–∞
            if analysis['phishing_indicators']['password_fields'] > 1:
                analysis['warnings'].append('–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ª–µ–π –¥–ª—è –≤–≤–æ–¥–∞ –ø–∞—Ä–æ–ª—è')
                analysis['is_dangerous'] = True

            if analysis['phishing_indicators']['hidden_elements'] > 5:
                analysis['warnings'].append('–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–Ω–æ–≥–æ —Å–∫—Ä—ã—Ç—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤')

            if analysis['phishing_indicators']['suspicious_scripts'] > 0:
                analysis['warnings'].append('–ù–∞–π–¥–µ–Ω—ã –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ JavaScript-—Ñ—É–Ω–∫—Ü–∏–∏')

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            analysis['domain_checks'] = check_domain_and_ssl(url)
            analysis['content_analysis'] = analyze_content(soup)
            analysis['reputation'] = check_reputation(url)
            analysis['heuristics'] = heuristic_checks(url, driver.page_source)

            # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–≥–∏–∫—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            if (analysis['reputation'].get('is_blacklisted', False) or
                    analysis['heuristics'].get('typosquatting', False) or
                    analysis['content_analysis'].get('fake_login_forms', 0) > 0):
                analysis['is_dangerous'] = True

            logger.info(
                f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {url}. "
                f"–ù–∞–π–¥–µ–Ω–æ {len(analysis['warnings'])} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π."
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {str(e)}", exc_info=True)
            analysis['warnings'].append(f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}')

        return analysis

    @staticmethod
    def calculate_safety_level(analysis: Dict[str, Any]) -> Tuple[str, str]:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞.

        Args:
            analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

        Returns:
            Tuple: (—É—Ä–æ–≤–µ–Ω—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —Ü–≤–µ—Ç–æ–≤–æ–π –∫–æ–¥)
        """
        score = 0

        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        if analysis['basic_checks']['has_ssl']:
            score += 1

        domain_age = analysis['domain_checks'].get('domain_age_days')
        if domain_age is not None:
            if domain_age > 365:
                score += 2
            elif domain_age > 30:
                score += 1

        # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        if analysis['reputation'].get('is_blacklisted', False):
            score -= 3

        if analysis['heuristics'].get('typosquatting', False):
            score -= 2

        if analysis['content_analysis'].get('fake_login_forms', 0) > 0:
            score -= 2

        if analysis['heuristics'].get('brand_impersonation', False):
            score -= 1

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –∫—Ä–∞–π–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        final_score = max(min(score, 4), -3)

        if final_score >= 3:
            return "üü¢ –ë–µ–∑–æ–ø–∞—Å–Ω–æ", "green"
        elif 1 <= final_score < 3:
            return "üü° –£—Å–ª–æ–≤–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ", "yellow"
        elif -1 <= final_score < 1:
            return "üü† –†–∏—Å–∫–æ–≤–∞–Ω–Ω–æ", "orange"
        else:
            return "üî¥ –û–ø–∞—Å–Ω–æ—Å—Ç—å!", "red"